{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ab5819",
   "metadata": {},
   "source": [
    "# Qlearning\n",
    "\n",
    " $$\n",
    " Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n",
    " $$\n",
    " \n",
    " 其中：\n",
    " - $Q(s_t, a_t)$：当前状态$s_t$下采取动作$a_t$的Q值\n",
    " - $\\alpha$：学习率，控制Q值的更新幅度\n",
    " - $r_{t+1}$：在状态$s_t$下采取动作$a_t$后获得的即时奖励\n",
    " - $\\gamma$：折扣因子，权衡未来奖励的影响\n",
    " - $\\max_{a} Q(s_{t+1}, a)$：在新状态$s_{t+1}$下所有可能动作的最大Q值\n",
    "\n",
    "# Sarsa\n",
    " \n",
    "  $$\n",
    "  Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n",
    "  $$\n",
    " \n",
    " 其中：\n",
    "  - $Q(s_t, a_t)$：当前状态$s_t$下采取动作$a_t$的Q值\n",
    "  - $\\alpha$：学习率，控制Q值的更新幅度\n",
    "  - $r_{t+1}$：在状态$s_t$下采取动作$a_t$后获得的即时奖励\n",
    "  - $\\gamma$：折扣因子，权衡未来奖励的影响\n",
    "  - $Q(s_{t+1}, a_{t+1})$：在新状态$s_{t+1}$下实际所选动作$a_{t+1}$的Q值\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a739610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import stat\n",
    "from random import uniform\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "state  1 2 3 4 5\n",
    "left   0 0 0 0 0\n",
    "right  0 0 0 1 0\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "N_STATES = 6  # 1维世界宽度\n",
    "ACTIONS = [\"left\", \"right\"] # 可用动作\n",
    "EPSILON = 0.9 # 贪婪度 greedy\n",
    "ALPHA = 0.1 # 学习率\n",
    "GAMMA = 0.9 # 奖励衰减值\n",
    "MAX_EPISODES = 10 # 最大回合数\n",
    "FRESH_TIME = 0.2 # 移动间隔时间\n",
    "\n",
    "def buildQTable(n_states: int, actions: list):\n",
    "    \"\"\"\n",
    "    创建一个 Q 表（Q-Table）来存储每个状态下所有可能动作的价值。\n",
    "    \n",
    "    参数:\n",
    "        n_states (int): 环境中的状态数量。\n",
    "        actions (list): 可用的动作列表。\n",
    "        \n",
    "    返回:\n",
    "        pd.DataFrame: 一个全零初始化的 Q 表，行表示状态，列表示动作。\n",
    "    \"\"\"\n",
    "    table = pd.DataFrame(np.zeros((n_states, len(actions))), columns=actions)\n",
    "    print(table)\n",
    "    return table\n",
    "\n",
    "def chooseAction(state: int, q_table: pd.DataFrame) -> str:\n",
    "  state_actions = q_table.iloc[state, :]\n",
    "  if (np.random.uniform() > EPSILON) or (state_actions.all() == 0):\n",
    "    action_name = np.random.choice(ACTIONS)\n",
    "  else:\n",
    "    action_name = state_actions.idxmax()\n",
    "  return action_name\n",
    "\n",
    "def getEnvFeedback(S, A):\n",
    "  if A == \"right\":\n",
    "    if (S == N_STATES - 1):\n",
    "      S_ = S\n",
    "      R = 1\n",
    "      Done = 1\n",
    "    else:\n",
    "      S_ = S + 1\n",
    "      R = 0\n",
    "      Done = 0\n",
    "  else:\n",
    "    R = 0\n",
    "    Done = 0\n",
    "    if S == 0:\n",
    "      S_ = S\n",
    "    else:\n",
    "      S_ = S - 1\n",
    "\n",
    "  return S_, R, Done\n",
    "\n",
    "def UpdateEnv(S, Done, episode, step_counter):\n",
    "  env_list = [\"-\"]*(N_STATES-1) + [\"T\"]\n",
    "  if Done == 1:\n",
    "    interaction = \"Episode %s: total_steps=%s\" % (episode+1, step_counter)\n",
    "    print(\"\\r{}\".format(interaction), end=\"\")\n",
    "    time.sleep(1)\n",
    "    print(\"\\r                             \", end=\"\")\n",
    "  else:\n",
    "    env_list[S] = \"O\"\n",
    "    interaction = \"\".join(env_list)\n",
    "    print(\"\\r{}\".format(interaction), end=\"\")\n",
    "    time.sleep(FRESH_TIME)\n",
    "  \n",
    "def RLLoop(learner: str = \"Qlearn\"):\n",
    "  print(\"learner: \", learner)\n",
    "  q_table = buildQTable(N_STATES, ACTIONS)\n",
    "  for episode in range(MAX_EPISODES):\n",
    "    step_counter = 0\n",
    "    S = 0\n",
    "    A = chooseAction(S, q_table=q_table)\n",
    "    Done = 0\n",
    "    is_terminated = False\n",
    "    UpdateEnv(S, Done, episode, step_counter)\n",
    "\n",
    "    while not is_terminated:\n",
    "      if learner == \"Qlearn\":  # (s, a, r, s_)\n",
    "        A = chooseAction(S, q_table=q_table)\n",
    "        S_, R, Done = getEnvFeedback(S, A)\n",
    "        q_predict = q_table.loc[S, A]\n",
    "        if Done != 1:\n",
    "            q_target = R+GAMMA*q_table.iloc[S_, :].max()\n",
    "        else:\n",
    "            q_target = R\n",
    "            is_terminated = True\n",
    "        q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # q_table 更新\n",
    "        S = S_  # 探索者移动到下一个 state\n",
    "      elif learner == \"Sarsa\":  # (s, a, r, s_, a_)\n",
    "        S_, R, Done = getEnvFeedback(S, A)\n",
    "        A_ = chooseAction(S_, q_table=q_table)\n",
    "        q_predict = q_table.loc[S, A]\n",
    "        if Done != 1:\n",
    "          q_target = R+GAMMA*q_table.loc[S_, A_]\n",
    "        else:\n",
    "          q_target = R\n",
    "          is_terminated = True\n",
    "        q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # q_table 更新\n",
    "        S = S_  # 探索者移动到下一个 state\n",
    "        A = A_\n",
    "      else:\n",
    "        print(\"\\n error: learner\")\n",
    "        return\n",
    "      UpdateEnv(S, Done, episode, step_counter+1)\n",
    "\n",
    "      step_counter += 1\n",
    "    # print(q_table)\n",
    "  return q_table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b51819b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learner:  Qlearn\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "                             \n",
      "Q-table:\n",
      "\n",
      "           left     right\n",
      "0  1.488035e-07  0.000209\n",
      "1  4.475419e-06  0.001912\n",
      "2  5.904900e-07  0.013794\n",
      "3  6.631859e-05  0.080937\n",
      "4  8.695844e-03  0.244485\n",
      "5  2.268000e-03  0.651322\n"
     ]
    }
   ],
   "source": [
    "q_table = RLLoop(\"Qlearn\")\n",
    "print('\\r\\nQ-table:\\n')\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "353bf086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learner:  Sarsa\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "                             \n",
      "Q-table:\n",
      "\n",
      "           left     right\n",
      "0  5.314410e-08  0.000087\n",
      "1  2.375749e-07  0.001026\n",
      "2  1.501268e-05  0.008937\n",
      "3  2.979772e-04  0.056510\n",
      "4  5.904900e-05  0.237124\n",
      "5  7.290000e-04  0.651322\n"
     ]
    }
   ],
   "source": [
    "q_table = RLLoop(\"Sarsa\")\n",
    "print('\\r\\nQ-table:\\n')\n",
    "print(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
